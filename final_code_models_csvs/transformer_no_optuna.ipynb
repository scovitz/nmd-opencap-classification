{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8ce73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, LayerNormalization, LSTM, Dropout, Input, GlobalAveragePooling1D, BatchNormalization, Masking\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import legacy as legacy_optimizers\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from sklearn.metrics import classification_report\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import optuna\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve, average_precision_score, classification_report, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a97b920",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.mixed_precision import set_global_policy\n",
    "set_global_policy('mixed_float16')  # May improve speed on some CPUs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc89001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for reading OpenCap outputs \n",
    "\n",
    "def read_trc(fpath):\n",
    "    # read metadata in file header\n",
    "    df_meta = pd.read_csv(fpath, delimiter='\\t', header=0, skiprows=1, nrows=1)\n",
    "    meta = df_meta.iloc[0].to_dict()\n",
    "    fps = meta['DataRate']\n",
    "\n",
    "    # read marker location names\n",
    "    markers_df = pd.read_csv(fpath, delimiter='\\t', header=None, skiprows=3, nrows=1)\n",
    "    markers = markers_df.iloc[0].dropna().to_numpy()[2:]\n",
    "\n",
    "    # read marker XYZ locations\n",
    "    df = pd.read_csv(fpath, delimiter='\\t', header=0, skiprows=4)\n",
    "    df.rename(columns=dict(zip(df.columns[:2], ('n', 't'))), inplace=True)\n",
    "    df.dropna(how='all', axis=1, inplace=True)\n",
    "\n",
    "    N = df.shape[0]\n",
    "    M = len(markers)\n",
    "    xyz = df.iloc[:,2:].to_numpy().reshape((N, M, 3))\n",
    "    xyz[:,:,[0,1,2]] = xyz[:,:,[2,1,0]]\n",
    "\n",
    "    return fps, markers, xyz\n",
    "\n",
    "\n",
    "def read_mot(fpath):\n",
    "    with open(fpath, 'r') as f:\n",
    "        # count = 0\n",
    "        line = f.readline().strip()\n",
    "        # while line and line.strip() != 'endheader':\n",
    "        while line.lower() != 'endheader':\n",
    "            line = f.readline().strip()\n",
    "            # count += 1\n",
    "\n",
    "        # df = pd.read_csv(f, delimiter='\\t', header=0, skiprows=count-3)\n",
    "        df = pd.read_csv(f, delimiter='\\t', header=0)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084ce729",
   "metadata": {},
   "outputs": [],
   "source": [
    "def z_score_normalize(data_list):\n",
    "    \"\"\"\n",
    "    Applies z-score normalization to a list of time series arrays.\n",
    "    Arguments:\n",
    "        data_list: List of numpy arrays, each of shape (num_timesteps, num_features).\n",
    "    Returns:\n",
    "        normalized_data: List of z-score normalized arrays.\n",
    "        mean: Mean of each feature across all time series.\n",
    "        std: Standard deviation of each feature across all time series.\n",
    "    \"\"\"\n",
    "    # Concatenate all data to compute global mean and std\n",
    "    all_data = np.concatenate(data_list, axis=0)\n",
    "    mean = np.mean(all_data, axis=0)\n",
    "    std = np.std(all_data, axis=0)\n",
    "    \n",
    "    # Normalize each array\n",
    "    normalized_data = [(data - mean) / (std + 1e-8) for data in data_list]\n",
    "    return normalized_data, mean, std\n",
    "\n",
    "def prepare_timeseries_input(subject_dict):\n",
    "    \"\"\"\n",
    "    Combines multiple time series for each subject along the time axis, applies z-score normalization,\n",
    "    and pads them for transformer input.\n",
    "    Arguments:\n",
    "        subject_dict: Dictionary where keys are subject IDs and values are lists of 6 numpy arrays.\n",
    "        Each numpy array corresponds to a time series with shape (num_timesteps, num_features).\n",
    "    Returns:\n",
    "        padded_input: A numpy array of shape (num_subjects, max_timesteps, num_features).\n",
    "        subject_ids: List of subject IDs in the order they appear in the padded_input array.\n",
    "        mean: Mean of features used for z-score normalization.\n",
    "        std: Standard deviation of features used for z-score normalization.\n",
    "    \"\"\"\n",
    "    # Sort the dictionary by keys to ensure consistent ordering\n",
    "    subject_ids = sorted(subject_dict.keys())\n",
    "    \n",
    "    # Extract and concatenate the 6 time series for each subject along the time axis\n",
    "    concatenated_series = [\n",
    "        np.concatenate(subject_dict[subject_id], axis=0) for subject_id in subject_ids\n",
    "    ]\n",
    "    \n",
    "    # Apply z-score normalization to the concatenated series\n",
    "    normalized_series, mean, std = z_score_normalize(concatenated_series)\n",
    "    \n",
    "    # Determine the maximum length for padding\n",
    "    max_length = max(series.shape[0] for series in normalized_series)\n",
    "    \n",
    "    # Pad all normalized time series to the same length\n",
    "    padded_input = pad_sequences(normalized_series, maxlen=max_length, padding=\"post\", dtype=\"float32\")\n",
    "    \n",
    "    return padded_input, subject_ids, mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186210db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_for_participant_trial(class_df, pid, trial):\n",
    "    \"\"\"\n",
    "    Fetches the label for a given participant and trial based on class_info.csv.\n",
    "    \n",
    "    Parameters:\n",
    "        pid (str): The participant ID\n",
    "        trial (str): The trial ID\n",
    "    \n",
    "    Returns:\n",
    "        label (int): Classification label (0 or 1).\n",
    "    \"\"\"\n",
    "    label = class_df.loc[class_df['ID'] == pid, 'Class'].values[0]\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74739701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the input data\n",
    "\n",
    "# Directory containing dataset files\n",
    "root_dir = '/Users/scovitz/datadir/opencap_data'  # path to root data dir \n",
    "labels_csv = '/Users/scovitz/datadir/class_info.csv' # csv with classification labels\n",
    "class_df = pd.read_csv(labels_csv)\n",
    "\n",
    "time_series_data = {}  # dictionary of pid, timeseries pairs \n",
    "\n",
    "expected_columns = [\n",
    "        'pelvis_tilt', 'pelvis_list', 'pelvis_rotation', 'pelvis_tx',\n",
    "        'pelvis_ty', 'pelvis_tz', 'hip_flexion_r', 'hip_adduction_r',\n",
    "        'hip_rotation_r', 'knee_angle_r', 'ankle_angle_r',\n",
    "        'subtalar_angle_r', 'mtp_angle_r', 'hip_flexion_l', 'hip_adduction_l',\n",
    "        'hip_rotation_l', 'knee_angle_l', 'ankle_angle_l',\n",
    "        'subtalar_angle_l', 'mtp_angle_l', 'lumbar_extension', 'lumbar_bending',\n",
    "        'lumbar_rotation', 'arm_flex_r', 'arm_add_r', 'arm_rot_r',\n",
    "        'elbow_flex_r', 'pro_sup_r', 'arm_flex_l', 'arm_add_l', 'arm_rot_l',\n",
    "        'elbow_flex_l', 'pro_sup_l'\n",
    "    ]\n",
    "\n",
    "# Loop through all .mot files\n",
    "for i in range(129):\n",
    "        # ensure going in order!\n",
    "        num = str(i+1).zfill(3)\n",
    "\n",
    "        for path in Path(root_dir).rglob(f\"P{num}*/**/*.mot\"): \n",
    "            \n",
    "            # remove upper extremity trials \n",
    "            if \"brooke\" in str(path) or \"curls\" in str(path) or \"arm_rom\" in str(path):\n",
    "                continue \n",
    "            \n",
    "            trial = str(path).split('/')[-3]\n",
    "            pid = str(path).split('/')[-4]\n",
    "            activity = str(path).split('/')[-2]\n",
    "\n",
    "            # Load the .mot file, get data, and cut first 200 timesteps (before activity starts)\n",
    "            data = read_mot(str(path))\n",
    "            data = data[200:]\n",
    "\n",
    "\n",
    "            # Ensure the file has the expected columns\n",
    "            if not all(col in data.columns for col in expected_columns + ['time']):\n",
    "                raise ValueError(f\"File {path} does not contain the expected columns.\")\n",
    "            \n",
    "            # Drop the 'time' column and keep only relevant features\n",
    "            data = np.array(data[expected_columns])\n",
    "            \n",
    "            if pid not in time_series_data:\n",
    "                time_series_data[pid + '_' + trial] = []\n",
    "\n",
    "            time_series_data[(pid + '_' + trial)].append(data) # value = list of timeseries arrays of num_timesteps x num_feats\n",
    "            print(data.shape, pid, trial, str(path))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58503d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(time_series_data)\n",
    "time_series_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4e31c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the transformer input\n",
    "timeseries_input, subject_ids, mean, std = prepare_timeseries_input(time_series_data)\n",
    "\n",
    "# Check the results\n",
    "print(\"Timeseries Input Shape:\", timeseries_input.shape)  # (162, max_timesteps, 35)\n",
    "print(\"Subject IDs:\", subject_ids[:5])  # subject ID ordering, should be in order! \n",
    "print(\"Mean Shape:\", mean.shape)  # shape of mean: (35,)\n",
    "print(\"Std Shape:\", std.shape)  # shape of std: (35,)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34837cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to a numpy array\n",
    "labels = []\n",
    "for sid in subject_ids:\n",
    "    pid = sid.split('_')[0]\n",
    "    trial = sid.split('_')[1]\n",
    "    label = get_label_for_participant_trial(class_df, pid, trial)\n",
    "    labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073af037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to numpy array\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155da38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train, validation, and test sets - 80, 10, 10\n",
    "def split_data(X, y, test_size=0.1, val_size=0.1):\n",
    "    \"\"\"\n",
    "    Splits the data into training, validation, and testing sets.\n",
    "    Arguments:\n",
    "        X: Input features (numpy array).\n",
    "        y: Labels (numpy array).\n",
    "        test_size: Proportion of data for the test set.\n",
    "        val_size: Proportion of data for the validation set from the training set.\n",
    "    Returns:\n",
    "        X_train, X_val, X_test, y_train, y_val, y_test\n",
    "    \"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=val_size, random_state=42)\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "\n",
    "# Split data\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = split_data(timeseries_input, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41eb63b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer Architecture\n",
    "\n",
    "class MultiHeadSelfAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, regularizer=None):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.projection_dim = embed_dim // num_heads\n",
    "        self.query_dense = Dense(embed_dim, kernel_regularizer=regularizer)\n",
    "        self.key_dense = Dense(embed_dim, kernel_regularizer=regularizer)\n",
    "        self.value_dense = Dense(embed_dim, kernel_regularizer=regularizer)\n",
    "        self.combine_heads = Dense(embed_dim, kernel_regularizer=regularizer)\n",
    "\n",
    "    def attention(self, query, key, value):\n",
    "        score = tf.matmul(query, key, transpose_b=True) / tf.sqrt(tf.cast(self.projection_dim, query.dtype))\n",
    "        weights = tf.nn.softmax(score, axis=-1)\n",
    "        return tf.matmul(weights, value), weights\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        inputs = tf.cast(inputs, dtype=tf.float32)\n",
    "\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        query = self.query_dense(inputs)\n",
    "        key = self.key_dense(inputs)\n",
    "        value = self.value_dense(inputs)\n",
    "\n",
    "        query = self.split_heads(query, batch_size)\n",
    "        key = self.split_heads(key, batch_size)\n",
    "        value = self.split_heads(value, batch_size)\n",
    "\n",
    "        attention, _ = self.attention(query, key, value)\n",
    "        attention = tf.transpose(attention, perm=[0, 2, 1, 3])\n",
    "        concat_attention = tf.reshape(attention, (batch_size, -1, self.embed_dim))\n",
    "        return self.combine_heads(concat_attention)\n",
    "\n",
    "class TransformerBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1, regularizer=None):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = MultiHeadSelfAttention(embed_dim, num_heads, regularizer)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            Dense(ff_dim, activation='relu', kernel_regularizer=regularizer),\n",
    "            Dense(embed_dim, kernel_regularizer=regularizer)\n",
    "        ])\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "# Build Transformer Model \n",
    "def build_transformer_model(input_shape, num_heads=4, embed_dim=64, ff_dim=128, num_blocks=2, l2_reg=1e-3):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    # Embedding layer with regularization\n",
    "    x = Dense(embed_dim, kernel_regularizer=l2(l2_reg))(inputs)\n",
    "    \n",
    "    # Add Transformer blocks with regularization\n",
    "    for _ in range(num_blocks):\n",
    "        x = TransformerBlock(embed_dim, num_heads, ff_dim, rate=0.3, regularizer=l2(l2_reg))(x, training=True)\n",
    "\n",
    "    # Global pooling and output layer\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    x = Dropout(0.5)(x) # INCREASED FROM 0.3\n",
    "    outputs = Dense(1, activation=\"sigmoid\", kernel_regularizer=l2(l2_reg))(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    # learning rate decreaesd from 0.001\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d119c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the Transformer model\n",
    "input_shape = (timeseries_input.shape[1], timeseries_input.shape[2])  # timesteps, features\n",
    "transformer_model = build_transformer_model(input_shape)\n",
    "\n",
    "# Print a summary of the model\n",
    "transformer_model.summary()\n",
    "\n",
    "# Early stopping\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=50,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "\n",
    "# Compute class weights to balance the dataset\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weight_dict = dict(enumerate(class_weights))\n",
    "\n",
    "lr_scheduler = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,  # Reduce the learning rate by half\n",
    "    patience=5,  # Wait for 5 epochs without improvement\n",
    "    min_lr=1e-6  # Minimum learning rate\n",
    ")\n",
    "\n",
    "history = transformer_model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=70,\n",
    "    batch_size=16,\n",
    "    callbacks=[early_stopping], # Trying bringing back early stopping or lr_scheduler\n",
    "    class_weight=class_weight_dict,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "train_accuracy = history.history['accuracy']\n",
    "val_accuracy = history.history['val_accuracy']\n",
    "\n",
    "# Print final training and validation accuracy\n",
    "print(f\"Final Training Accuracy: {train_accuracy[-1]:.4f}\")\n",
    "print(f\"Final Validation Accuracy: {val_accuracy[-1]:.4f}\")\n",
    "\n",
    "# Evaluate the model on test data\n",
    "test_loss, test_accuracy = transformer_model.evaluate(X_test, y_test, verbose=1)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Get predictions\n",
    "y_pred = (transformer_model.predict(X_test) > 0.5).astype(\"int32\")  # Threshold at 0.5\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, digits=4))\n",
    "\n",
    "test_f1 = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "print(\"Weighted F1 Score: \", test_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afb4904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Training Loss\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss Over Epochs - Kinematic Timeseries')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7168e60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print final training and validation accuracy\n",
    "train_accuracy = history.history['accuracy']\n",
    "val_accuracy = history.history['val_accuracy']\n",
    "print(f\"Final Training Accuracy: {train_accuracy[-1]:.4f}\")\n",
    "print(f\"Final Validation Accuracy: {val_accuracy[-1]:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "# Evaluate the model on test data\n",
    "test_loss, test_accuracy = transformer_model.evaluate(X_test, y_test, verbose=1)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Get predictions\n",
    "y_pred_prob = transformer_model.predict(X_test).ravel()  # Predicted probabilities\n",
    "y_pred = (y_pred_prob > 0.5).astype(\"int32\")  # Threshold at 0.5\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, digits=4))\n",
    "\n",
    "weighted_recall = classification_report(y_test, (y_pred_prob > 0.5).astype(int), output_dict=True)['weighted avg']['recall']\n",
    "weighted_precision = classification_report(y_test, (y_pred_prob > 0.5).astype(int), output_dict=True)['weighted avg']['precision']\n",
    "\n",
    "print('weighted average precision: ', weighted_precision)\n",
    "print(\"average recall: \", weighted_recall)\n",
    "\n",
    "\n",
    "# Compute and print Weighted F1 Score\n",
    "test_f1 = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "print(\"Weighted F1 Score: \", test_f1)\n",
    "\n",
    "# ROC-AUC and Precision-Recall Curves\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_pred_prob)\n",
    "average_precision = average_precision_score(y_test, y_pred_prob)\n",
    "\n",
    "# Plot ROC Curve\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.2f})', color='blue')\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Chance')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "# Plot Precision-Recall Curve\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(recall, precision, label=f'PR curve (AUPRC = {average_precision:.2f})', color='green')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall (PR) Curve')\n",
    "plt.legend(loc='lower left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print ROC-AUC and PR-AUC scores\n",
    "print(f\"\\nFinal ROC-AUC: {roc_auc:.4f}\")\n",
    "print(f\"Final Precision-Recall AUC (AUPRC): {average_precision:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
