{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8ce73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Dense, LSTM, LayerNormalization, Dropout, Input, GlobalAveragePooling1D, Layer Normalization, BatchNormalization, Masking\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import legacy as legacy_optimizers\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from sklearn.metrics import classification_report\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve, average_precision_score, classification_report, f1_score\n",
    "import optuna\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a97b920",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.mixed_precision import set_global_policy\n",
    "set_global_policy('mixed_float16')  # May improve speed on some CPUs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57464793",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.mixed_precision import set_global_policy\n",
    "set_global_policy('mixed_float16')  # May improve speed on some CPUs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc89001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for reading OpenCap outputs \n",
    "\n",
    "def read_trc(fpath):\n",
    "    # read metadata in file header\n",
    "    df_meta = pd.read_csv(fpath, delimiter='\\t', header=0, skiprows=1, nrows=1)\n",
    "    meta = df_meta.iloc[0].to_dict()\n",
    "    fps = meta['DataRate']\n",
    "\n",
    "    # read marker location names\n",
    "    markers_df = pd.read_csv(fpath, delimiter='\\t', header=None, skiprows=3, nrows=1)\n",
    "    markers = markers_df.iloc[0].dropna().to_numpy()[2:]\n",
    "\n",
    "    # read marker XYZ locations\n",
    "    df = pd.read_csv(fpath, delimiter='\\t', header=0, skiprows=4)\n",
    "    df.rename(columns=dict(zip(df.columns[:2], ('n', 't'))), inplace=True)\n",
    "    df.dropna(how='all', axis=1, inplace=True)\n",
    "\n",
    "    N = df.shape[0]\n",
    "    M = len(markers)\n",
    "    xyz = df.iloc[:,2:].to_numpy().reshape((N, M, 3))\n",
    "    xyz[:,:,[0,1,2]] = xyz[:,:,[2,1,0]]\n",
    "\n",
    "    return fps, markers, xyz\n",
    "\n",
    "\n",
    "def read_mot(fpath):\n",
    "    with open(fpath, 'r') as f:\n",
    "        # count = 0\n",
    "        line = f.readline().strip()\n",
    "        # while line and line.strip() != 'endheader':\n",
    "        while line.lower() != 'endheader':\n",
    "            line = f.readline().strip()\n",
    "            # count += 1\n",
    "\n",
    "        # df = pd.read_csv(f, delimiter='\\t', header=0, skiprows=count-3)\n",
    "        df = pd.read_csv(f, delimiter='\\t', header=0)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084ce729",
   "metadata": {},
   "outputs": [],
   "source": [
    "def z_score_normalize(data_list):\n",
    "    \"\"\"\n",
    "    Applies z-score normalization to a list of time series arrays.\n",
    "    Arguments:\n",
    "        data_list: List of numpy arrays, each of shape (num_timesteps, num_features).\n",
    "    Returns:\n",
    "        normalized_data: List of z-score normalized arrays.\n",
    "        mean: Mean of each feature across all time series.\n",
    "        std: Standard deviation of each feature across all time series.\n",
    "    \"\"\"\n",
    "    # Concatenate all data to compute global mean and std\n",
    "    all_data = np.concatenate(data_list, axis=0)\n",
    "    mean = np.mean(all_data, axis=0)\n",
    "    std = np.std(all_data, axis=0)\n",
    "    \n",
    "    # Normalize each array\n",
    "    normalized_data = [(data - mean) / (std + 1e-8) for data in data_list]\n",
    "    return normalized_data, mean, std\n",
    "\n",
    "def prepare_timeseries_input(subject_dict):\n",
    "    \"\"\"\n",
    "    Combines multiple time series for each subject along the time axis, applies z-score normalization,\n",
    "    and pads them for transformer input.\n",
    "    Arguments:\n",
    "        subject_dict: Dictionary where keys are subject IDs and values are lists of 6 numpy arrays.\n",
    "        Each numpy array corresponds to a time series with shape (num_timesteps, num_features).\n",
    "    Returns:\n",
    "        padded_input: A numpy array of shape (num_subjects, max_timesteps, num_features).\n",
    "        subject_ids: List of subject IDs in the order they appear in the padded_input array.\n",
    "        mean: Mean of features used for z-score normalization.\n",
    "        std: Standard deviation of features used for z-score normalization.\n",
    "    \"\"\"\n",
    "    # Sort the dictionary by keys to ensure consistent ordering\n",
    "    subject_ids = sorted(subject_dict.keys())\n",
    "    \n",
    "    # Extract and concatenate the 6 time series for each subject along the time axis\n",
    "    concatenated_series = [\n",
    "        np.concatenate(subject_dict[subject_id], axis=0) for subject_id in subject_ids\n",
    "    ]\n",
    "    \n",
    "    # Apply z-score normalization to the concatenated series\n",
    "    normalized_series, mean, std = z_score_normalize(concatenated_series)\n",
    "    \n",
    "    # Determine the maximum length for padding\n",
    "    max_length = max(series.shape[0] for series in normalized_series)\n",
    "    \n",
    "    # Pad all normalized time series to the same length\n",
    "    padded_input = pad_sequences(normalized_series, maxlen=max_length, padding=\"post\", dtype=\"float32\")\n",
    "    \n",
    "    return padded_input, subject_ids, mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186210db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_for_participant_trial(class_df, pid, trial):\n",
    "    \"\"\"\n",
    "    Fetches the label for a given participant based on class_info.csv.\n",
    "    \n",
    "    Parameters:\n",
    "        pid (str): The participant ID\n",
    "        trial (str): The trial ID\n",
    "    \n",
    "    Returns:\n",
    "        label(int): Classification label (0 or 1)\n",
    "    \"\"\"\n",
    "    label = class_df.loc[class_df['ID'] == pid, 'Class'].values[0]\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74739701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing .mot files\n",
    "root_dir = '/Users/scovitz/datadir/opencap_data'  # path to root data dir \n",
    "labels_csv = '/Users/scovitz/datadir/class_info.csv' # csv with classification labels\n",
    "class_df = pd.read_csv(labels_csv) \n",
    "\n",
    "time_series_data = {}  # dictionary of pid, timeseries pairs \n",
    "\n",
    "expected_columns = [\n",
    "        'pelvis_tilt', 'pelvis_list', 'pelvis_rotation', 'pelvis_tx',\n",
    "        'pelvis_ty', 'pelvis_tz', 'hip_flexion_r', 'hip_adduction_r',\n",
    "        'hip_rotation_r', 'knee_angle_r', 'ankle_angle_r',\n",
    "        'subtalar_angle_r', 'mtp_angle_r', 'hip_flexion_l', 'hip_adduction_l',\n",
    "        'hip_rotation_l', 'knee_angle_l', 'ankle_angle_l',\n",
    "        'subtalar_angle_l', 'mtp_angle_l', 'lumbar_extension', 'lumbar_bending',\n",
    "        'lumbar_rotation', 'arm_flex_r', 'arm_add_r', 'arm_rot_r',\n",
    "        'elbow_flex_r', 'pro_sup_r', 'arm_flex_l', 'arm_add_l', 'arm_rot_l',\n",
    "        'elbow_flex_l', 'pro_sup_l'\n",
    "    ]\n",
    "\n",
    "# Loop through all .mot files\n",
    "for i in range(129):\n",
    "        # ensure going in order!\n",
    "        num = str(i+1).zfill(3)\n",
    "\n",
    "        for path in Path(root_dir).rglob(f\"P{num}*/**/*.mot\"): \n",
    "            \n",
    "            # remove upper extremity trials \n",
    "            if \"brooke\" in str(path) or \"curls\" in str(path) or \"arm_rom\" in str(path):\n",
    "                continue \n",
    "            \n",
    "            trial = str(path).split('/')[-3]\n",
    "            pid = str(path).split('/')[-4]\n",
    "            activity = str(path).split('/')[-2]\n",
    "            #print(str(path))\n",
    "            # Load the .mot file\n",
    "            \n",
    "            data = read_mot(str(path))\n",
    "            data = data[200:]\n",
    "            #print(data.shape)\n",
    "\n",
    "            # Ensure the file has the expected columns\n",
    "            if not all(col in data.columns for col in expected_columns + ['time']):\n",
    "                raise ValueError(f\"File {path} does not contain the expected columns.\")\n",
    "            \n",
    "            # Drop the 'time' column and keep only relevant features\n",
    "            data = np.array(data[expected_columns])\n",
    "            \n",
    "            if pid not in time_series_data:\n",
    "                time_series_data[pid + '_' + trial] = []\n",
    "\n",
    "            time_series_data[(pid + '_' + trial)].append(data) # value = list of timeseries arrays of num_timesteps x num_feats\n",
    "            print(data.shape, pid, trial, str(path))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58503d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(time_series_data)\n",
    "time_series_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4e31c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the Transformer input\n",
    "timeseries_input, subject_ids, mean, std = prepare_timeseries_input(time_series_data)\n",
    "\n",
    "# Check the results\n",
    "print(\"Timeseries Input Shape:\", timeseries_input.shape)  # (162, max_timesteps, 35)\n",
    "print(\"Subject IDs:\", subject_ids[:5])  # subject ID ordering, should be in order! \n",
    "print(\"Mean Shape:\", mean.shape)  # shape of mean: (35,)\n",
    "print(\"Std Shape:\", std.shape)  # shape of std: (35,)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34837cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to a numpy array\n",
    "labels = []\n",
    "for sid in subject_ids:\n",
    "    pid = sid.split('_')[0]\n",
    "    trial = sid.split('_')[1]\n",
    "    label = get_label_for_participant_trial(class_df, pid, trial)\n",
    "    labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073af037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to numpy array\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac282738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loader\n",
    "def data_generator(X, y, batch_size):\n",
    "    num_samples = len(X)\n",
    "    while True:\n",
    "        for offset in range(0, num_samples, batch_size):\n",
    "            X_batch = X[offset:offset+batch_size]\n",
    "            y_batch = y[offset:offset+batch_size]\n",
    "            yield np.array(X_batch), np.array(y_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155da38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train, validation, and test sets - 80, 10, 10\n",
    "def split_data(X, y, test_size=0.1, val_size=0.1):\n",
    "    \"\"\"\n",
    "    Splits the data into training, validation, and testing sets.\n",
    "    Arguments:\n",
    "        X: Input features (numpy array).\n",
    "        y: Labels (numpy array).\n",
    "        test_size: Proportion of data for the test set.\n",
    "        val_size: Proportion of data for the validation set from the training set.\n",
    "    Returns:\n",
    "        X_train, X_val, X_test, y_train, y_val, y_test\n",
    "    \"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=val_size, random_state=42)\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "\n",
    "# Split data\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = split_data(timeseries_input, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9b28f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### TRYING OPTUNA FOR HYPERPARAM OPTIMIZATION #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cf3555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer Architecture \n",
    "\n",
    "class MultiHeadSelfAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, regularizer=None):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.projection_dim = embed_dim // num_heads\n",
    "        self.query_dense = Dense(embed_dim, kernel_regularizer=regularizer)\n",
    "        self.key_dense = Dense(embed_dim, kernel_regularizer=regularizer)\n",
    "        self.value_dense = Dense(embed_dim, kernel_regularizer=regularizer)\n",
    "        self.combine_heads = Dense(embed_dim, kernel_regularizer=regularizer)\n",
    "\n",
    "    def attention(self, query, key, value):\n",
    "        score = tf.matmul(query, key, transpose_b=True) / tf.sqrt(tf.cast(self.projection_dim, query.dtype))\n",
    "        weights = tf.nn.softmax(score, axis=-1)\n",
    "        return tf.matmul(weights, value), weights\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        inputs = tf.cast(inputs, dtype=tf.float32)\n",
    "\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        query = self.query_dense(inputs)\n",
    "        key = self.key_dense(inputs)\n",
    "        value = self.value_dense(inputs)\n",
    "\n",
    "        query = self.split_heads(query, batch_size)\n",
    "        key = self.split_heads(key, batch_size)\n",
    "        value = self.split_heads(value, batch_size)\n",
    "\n",
    "        attention, _ = self.attention(query, key, value)\n",
    "        attention = tf.transpose(attention, perm=[0, 2, 1, 3])\n",
    "        concat_attention = tf.reshape(attention, (batch_size, -1, self.embed_dim))\n",
    "        return self.combine_heads(concat_attention)\n",
    "\n",
    "class TransformerBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1, regularizer=None):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = MultiHeadSelfAttention(embed_dim, num_heads, regularizer)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            Dense(ff_dim, activation='relu', kernel_regularizer=regularizer),\n",
    "            Dense(embed_dim, kernel_regularizer=regularizer)\n",
    "        ])\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6d9c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable GPU for CPU-only machine\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weight_dict = dict(enumerate(class_weights))\n",
    "\n",
    "# Define the model structure\n",
    "def build_model(trial, input_shape):\n",
    "    # Suggest hyperparameters\n",
    "    embed_dim = trial.suggest_int(\"embed_dim\", 32, 128, step=32)  # Embedding dimension\n",
    "    num_heads = trial.suggest_int(\"num_heads\", 2, 8, step=2)      # Number of attention heads\n",
    "    ff_dim = trial.suggest_int(\"ff_dim\", 64, 256, step=64)        # Feedforward network dimension\n",
    "    num_blocks = trial.suggest_int(\"num_blocks\", 1, 3)            # Number of Transformer blocks\n",
    "    dropout_rate = trial.suggest_float(\"dropout_rate\", 0.1, 0.5)  # Dropout rate\n",
    "    l2_reg = trial.suggest_float(\"l2_reg\", 1e-4, 1e-2, log=True)  # L2 regularization strength\n",
    "\n",
    "    # Input layer\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    x = tf.keras.layers.Dense(embed_dim, kernel_regularizer=tf.keras.regularizers.l2(l2_reg))(inputs)\n",
    "    \n",
    "    # Add Transformer blocks\n",
    "    for _ in range(num_blocks):\n",
    "        x = TransformerBlock(embed_dim, num_heads, ff_dim, rate=dropout_rate, regularizer=tf.keras.regularizers.l2(l2_reg))(x, training=True)\n",
    "    \n",
    "    # Global pooling and output layer\n",
    "    x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "    x = tf.keras.layers.Dropout(dropout_rate)(x)\n",
    "    outputs = tf.keras.layers.Dense(1, activation=\"sigmoid\", kernel_regularizer=tf.keras.regularizers.l2(l2_reg))(x)\n",
    "\n",
    "    # Compile the model\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-4, 1e-2, log=True)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                  loss=\"binary_crossentropy\",\n",
    "                  metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "# Define the objective function\n",
    "def objective(trial):\n",
    "    epochs = trial.suggest_int(\"epochs\", 50, 200, step=50)  # Suggest number of epochs\n",
    "    model = build_model(trial, input_shape=(X_train.shape[1], X_train.shape[2]))\n",
    "\n",
    "    # Train the model with class weights\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=epochs,\n",
    "        batch_size=16,\n",
    "        class_weight=class_weight_dict,\n",
    "        verbose=0  # Suppress output\n",
    "    )\n",
    "    \n",
    "    # Return the validation loss (minimized by Optuna)\n",
    "    return min(history.history['val_loss'])\n",
    "\n",
    "# Create and run the Optuna study\n",
    "study = optuna.create_study(direction=\"minimize\")  # Minimize validation loss, could also try minimizing F1 score\n",
    "study.optimize(objective, n_trials=5)  # Adjust n_trials as needed\n",
    "\n",
    "# Display the best parameters\n",
    "#best_params = {'epochs': 70, 'embed_dim': 128, 'num_heads': 8, 'ff_dim': 256, 'num_blocks': 1, 'dropout_rate': 0.35062671547806423, 'l2_reg': 0.00010345073760147071, 'learning_rate': 0.00024969942836710864}\n",
    "#best_params = {'epochs': 300,'embed_dim': 64, 'ff_dim': 64, 'num_heads': 4, 'dropout_rate': 0.6,'l2_reg': 0.01,'learning_rate': 0.0005}\n",
    "\n",
    "print(\"Best hyperparameters:\", study.best_params)\n",
    "\n",
    "# Build and train the final model with the best parameters\n",
    "best_params = study.best_params\n",
    "final_model = build_model(\n",
    "    trial=optuna.trial.FixedTrial(best_params),  # Use fixed parameters\n",
    "    input_shape=(X_train.shape[1], X_train.shape[2])\n",
    ")\n",
    "history = final_model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=best_params['epochs'],  # Use optimal number of epochs\n",
    "    batch_size=16,\n",
    "    class_weight=class_weight_dict,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate the model on the training, validation, and test sets\n",
    "train_loss, train_accuracy = final_model.evaluate(X_train, y_train, verbose=0)\n",
    "val_loss, val_accuracy = final_model.evaluate(X_val, y_val, verbose=0)\n",
    "test_loss, test_accuracy = final_model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "# Predict to calculate F1 scores\n",
    "y_train_pred = (final_model.predict(X_train) > 0.5).astype(int)\n",
    "y_val_pred = (final_model.predict(X_val) > 0.5).astype(int)\n",
    "y_test_pred = (final_model.predict(X_test) > 0.5).astype(int)\n",
    "\n",
    "train_f1 = f1_score(y_train, y_train_pred, average=\"weighted\")\n",
    "val_f1 = f1_score(y_val, y_val_pred, average=\"weighted\")\n",
    "test_f1 = f1_score(y_test, y_test_pred, average=\"weighted\")\n",
    "\n",
    "# Print the final metrics\n",
    "print(f\"Final Training Loss: {train_loss:.4f}, Accuracy: {train_accuracy:.4f}, Weighted F1 Score: {train_f1:.4f}\")\n",
    "print(f\"Final Validation Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.4f}, Weighted F1 Score: {val_f1:.4f}\")\n",
    "print(f\"Final Test Loss: {test_loss:.4f}, Accuracy: {test_accuracy:.4f}, Weighted F1 Score: {test_f1:.4f}\")\n",
    "\n",
    "y_pred_prob = final_model.predict(X_test).ravel()  # Predicted probabilities\n",
    "y_pred = (y_pred_prob > 0.5).astype(\"int32\")  # Threshold at 0.5\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, digits=4))\n",
    "\n",
    "weighted_recall = classification_report(y_test, (y_pred_prob > 0.5).astype(int), output_dict=True)['weighted avg']['recall']\n",
    "weighted_precision = classification_report(y_test, (y_pred_prob > 0.5).astype(int), output_dict=True)['weighted avg']['precision']\n",
    "\n",
    "print('weighted average precision: ', weighted_precision)\n",
    "print(\"average recall: \", weighted_recall)\n",
    "\n",
    "# Plot the training and validation loss over all epochs\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history.history['loss'], label=\"Training Loss\")\n",
    "plt.plot(history.history['val_loss'], label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training and Validation Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# ROC-AUC and Precision-Recall Curves\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_pred_prob)\n",
    "average_precision = average_precision_score(y_test, y_pred_prob)\n",
    "\n",
    "# Plot ROC Curve\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.2f})', color='blue')\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Chance')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "# Plot Precision-Recall Curve\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(recall, precision, label=f'PR curve (AUPRC = {average_precision:.2f})', color='green')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall (PR) Curve')\n",
    "plt.legend(loc='lower left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print ROC-AUC and PR-AUC scores\n",
    "print(f\"\\nFinal ROC-AUC: {roc_auc:.4f}\")\n",
    "print(f\"Final Precision-Recall AUC (AUPRC): {average_precision:.4f}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
